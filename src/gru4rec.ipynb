{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YgoIF43pE2rF",
        "outputId": "3040f3e3-0232-4b0b-c979-11ceee238757"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "filepath='/content/drive/MyDrive/Colab Notebooks/data/Beauty_item_org_rank.txt'\n",
        "filepath2=\"/content/drive/MyDrive/Colab Notebooks/data/Beauty_item_var_rank.txt\""
      ],
      "metadata": {
        "id": "1I3rXEr4gCs7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Training with NDCG Early Stop**"
      ],
      "metadata": {
        "id": "IiG_aB759_DA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "torch.backends.cudnn.benchmark = True\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "\n",
        "def load_sequences(file_path, max_users=None):\n",
        "    sequences = []\n",
        "    with open(file_path, 'r') as f:\n",
        "        for i, line in enumerate(f):\n",
        "            if max_users and i >= max_users:\n",
        "                break\n",
        "            parts = list(map(int, line.strip().split()))\n",
        "            item_ids = parts[1:]  # skip user ID\n",
        "            if len(item_ids) >= 2:\n",
        "                sequences.append(item_ids)\n",
        "    return sequences\n",
        "\n",
        "def split_train_test(sequences):\n",
        "    train_seqs = []\n",
        "    test_data = []\n",
        "    for seq in sequences:\n",
        "        train_seqs.append(seq[:-1])\n",
        "        test_data.append((seq[:-1], seq[-1]))  # leave-one-out setup\n",
        "    return train_seqs, test_data\n",
        "\n",
        "class SequenceDataset(Dataset):\n",
        "    def __init__(self, inputs, targets):\n",
        "        self.inputs = [torch.tensor(seq, dtype=torch.long) for seq in inputs]\n",
        "        self.targets = torch.tensor(targets)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.inputs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.inputs[idx], self.targets[idx]\n",
        "\n",
        "def collate_fn(batch):\n",
        "    inputs, targets = zip(*batch)\n",
        "    inputs_padded = pad_sequence(inputs, batch_first=True)\n",
        "    return inputs_padded.to(device), torch.tensor(targets).to(device)\n",
        "\n",
        "\n",
        "class GRU4Rec(nn.Module):\n",
        "    def __init__(self, num_items, embedding_dim=128, hidden_dim=128):\n",
        "        super(GRU4Rec, self).__init__()\n",
        "        self.embedding = nn.Embedding(num_items, embedding_dim)\n",
        "        self.gru = nn.GRU(embedding_dim, hidden_dim, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_dim, num_items)\n",
        "\n",
        "    def forward(self, x):\n",
        "        emb = self.embedding(x)\n",
        "        _, h = self.gru(emb)\n",
        "        out = self.fc(h.squeeze(0))\n",
        "        return out\n",
        "\n",
        "\n",
        "def hit_ndcg_k(predictions, target, k):\n",
        "    k = min(k, len(predictions))\n",
        "    topk = predictions.argsort(descending=True)[:k]\n",
        "    hit = int(target in topk)\n",
        "    if target in topk:\n",
        "        ndcg = 1 / np.log2(topk.tolist().index(target.item()) + 2)\n",
        "    else:\n",
        "        ndcg = 0\n",
        "    return hit, ndcg\n",
        "\n",
        "def evaluate(model, test_data, k_list=[10]):\n",
        "    model.eval()\n",
        "    hits = {k: [] for k in k_list}\n",
        "    ndcgs = {k: [] for k in k_list}\n",
        "\n",
        "    for input_seq, target in test_data:\n",
        "        input_seq = torch.tensor(input_seq, dtype=torch.long, device=device).unsqueeze(0)\n",
        "        target = torch.tensor(target, dtype=torch.long, device=device)\n",
        "        with torch.no_grad():\n",
        "            prediction = model(input_seq).squeeze(0)\n",
        "\n",
        "        for k in k_list:\n",
        "            hit, ndcg = hit_ndcg_k(prediction, target, k)\n",
        "            hits[k].append(hit)\n",
        "            ndcgs[k].append(ndcg)\n",
        "\n",
        "    for k in k_list:\n",
        "        print(f\"Hit@{k}: {np.mean(hits[k]):.4f}, NDCG@{k}: {np.mean(ndcgs[k]):.4f}\")\n",
        "\n",
        "    return np.mean(ndcgs[10])  # return NDCG@10 for early stopping\n",
        "\n",
        "# ----------------------- Training with NDCG Early Stop -----------------------\n",
        "def train_model(filepath, embedding_dim=128, batch_size=128, epochs=100, max_users=None, patience=5):\n",
        "    print(f\"\\nLoading file: {filepath}\")\n",
        "    sequences = load_sequences(filepath, max_users)\n",
        "    train_seqs, test_data = split_train_test(sequences)\n",
        "    num_items = max(max(seq) for seq in sequences) + 1\n",
        "\n",
        "    # Prepare training input-target pairs\n",
        "    inputs = []\n",
        "    targets = []\n",
        "    for seq in train_seqs:\n",
        "        for i in range(1, len(seq)):\n",
        "            inputs.append(seq[:i])\n",
        "            targets.append(seq[i])\n",
        "\n",
        "    dataset = SequenceDataset(inputs, targets)\n",
        "    loader = DataLoader(dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
        "\n",
        "    model = GRU4Rec(num_items, embedding_dim, embedding_dim).to(device)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    best_ndcg = 0\n",
        "    patience_counter = 0\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "        for batch_inputs, batch_targets in loader:\n",
        "            logits = model(batch_inputs)\n",
        "            loss = criterion(logits, batch_targets)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        print(f\"Epoch {epoch+1}: Training Loss = {total_loss / len(loader):.4f}\")\n",
        "\n",
        "        # Evaluate on test set\n",
        "        current_ndcg = evaluate(model, test_data, k_list=[10, 20])\n",
        "\n",
        "        if current_ndcg > best_ndcg:\n",
        "            best_ndcg = current_ndcg\n",
        "            patience_counter = 0\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "            print(f\"NDCG did not improve. Early stopping counter: {patience_counter}/{patience}\")\n",
        "            if patience_counter >= patience:\n",
        "                print(\"\\nEarly stopping triggered based on NDCG!\")\n",
        "                break\n",
        "\n",
        "    print(\"\\nFinal Evaluation:\")\n",
        "    evaluate(model, test_data, k_list=[10, 20])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3nqMqk1HssKz",
        "outputId": "1211dade-5eda-450c-965d-e04808395724"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_model(filepath, embedding_dim=128, batch_size=128, epochs=350, max_users=None)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JP1_0KW_s8FT",
        "outputId": "11080861-6ca4-4121-80b2-ff1a9a384072"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Loading file: /content/drive/MyDrive/Colab Notebooks/data/Beauty_item_org_rank.txt\n",
            "Epoch 1: Training Loss = 8.8119\n",
            "Hit@10: 0.0228, NDCG@10: 0.0114\n",
            "Hit@20: 0.0358, NDCG@20: 0.0147\n",
            "Epoch 2: Training Loss = 8.2093\n",
            "Hit@10: 0.0316, NDCG@10: 0.0156\n",
            "Hit@20: 0.0512, NDCG@20: 0.0205\n",
            "Epoch 3: Training Loss = 7.8274\n",
            "Hit@10: 0.0361, NDCG@10: 0.0184\n",
            "Hit@20: 0.0579, NDCG@20: 0.0239\n",
            "Epoch 4: Training Loss = 7.4783\n",
            "Hit@10: 0.0423, NDCG@10: 0.0217\n",
            "Hit@20: 0.0643, NDCG@20: 0.0272\n",
            "Epoch 5: Training Loss = 7.1185\n",
            "Hit@10: 0.0443, NDCG@10: 0.0227\n",
            "Hit@20: 0.0677, NDCG@20: 0.0286\n",
            "Epoch 6: Training Loss = 6.7549\n",
            "Hit@10: 0.0438, NDCG@10: 0.0232\n",
            "Hit@20: 0.0672, NDCG@20: 0.0291\n",
            "Epoch 7: Training Loss = 6.3968\n",
            "Hit@10: 0.0450, NDCG@10: 0.0237\n",
            "Hit@20: 0.0668, NDCG@20: 0.0292\n",
            "Epoch 8: Training Loss = 6.0586\n",
            "Hit@10: 0.0427, NDCG@10: 0.0229\n",
            "Hit@20: 0.0645, NDCG@20: 0.0283\n",
            "NDCG did not improve. Early stopping counter: 1/5\n",
            "Epoch 9: Training Loss = 5.7464\n",
            "Hit@10: 0.0404, NDCG@10: 0.0219\n",
            "Hit@20: 0.0622, NDCG@20: 0.0274\n",
            "NDCG did not improve. Early stopping counter: 2/5\n",
            "Epoch 10: Training Loss = 5.4616\n",
            "Hit@10: 0.0388, NDCG@10: 0.0209\n",
            "Hit@20: 0.0585, NDCG@20: 0.0258\n",
            "NDCG did not improve. Early stopping counter: 3/5\n",
            "Epoch 11: Training Loss = 5.1995\n",
            "Hit@10: 0.0389, NDCG@10: 0.0205\n",
            "Hit@20: 0.0576, NDCG@20: 0.0252\n",
            "NDCG did not improve. Early stopping counter: 4/5\n",
            "Epoch 12: Training Loss = 4.9761\n",
            "Hit@10: 0.0364, NDCG@10: 0.0197\n",
            "Hit@20: 0.0544, NDCG@20: 0.0242\n",
            "NDCG did not improve. Early stopping counter: 5/5\n",
            "\n",
            "Early stopping triggered based on NDCG!\n",
            "\n",
            "Final Evaluation:\n",
            "Hit@10: 0.0364, NDCG@10: 0.0197\n",
            "Hit@20: 0.0544, NDCG@20: 0.0242\n"
          ]
        }
      ]
    }
  ]
}